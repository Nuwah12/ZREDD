# -*- coding: utf-8 -*-
"""HousingData_Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_YHlevjCjANXUCxT-okdD0Z7rAC4wrFi

# Preprocessing notebook for Zillow Housing Data
### Data is obtained from https://www.zillow.com/research/data/
#### Included datasets:
* Zillow Home Value Index (ZHVI) - separate datasets for prices by **Country(US)/Metro**, **State**, **County**, **City**, **ZIP Code**, and **Neighborhood**
* Zillow Home Value *Forecast* (ZHVF) - by **Country(US)/Metro** and **ZIP code**
* Zillow Observed Rent Index (ZORI) - by **Country(US)/Metro**, **County**, **City**, and **ZIP code**
* For-Sale Listings - by **Country(US)/Metro**
* Sales (Estimated number of properties sold in a month) - by **Country(US)/Metro**
* Market Heat Index (higher number $\rightarrow$ market favors sellers) - by **Country(US)/Metro**
* New Construction - by **Country(US)/Metro**
"""

# Mount Google Drive, data will be in /content/drive/MyDrive/Zillow_Housing_Data
from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd

### 'Melt' the dataframes so that 'time' is a single attribute
def preprocess_raw(df_dict, datecol_begin):
  regions_df = pd.DataFrame() # Initialize empty dataframe for region information
  for n,i in enumerate(df_dict):
    cols = df_dict[i].columns
    df_dict[i] = pd.melt(df_dict[i], id_vars=cols[0:datecol_begin[n]], value_vars=cols[datecol_begin[n]:]) # 'Melt' the dataframe so that we have a single 'time' attribute
    df_dict[i] = df_dict[i].dropna() # Drop rows with NA
    df_dict[i].columns = list(cols[0:datecol_begin[n]]) + ['date','value'] # Rename columns
    df_dict[i]['date'] = pd.to_datetime(df_dict[i]['date']) # Datetime
    print('cont')
    # Drop region-related columns - this information will be retained in the Regions table
    region_cols = df_dict[i].columns[0:datecol_begin[n]]
    print(region_cols)
    region_df = df_dict[i][region_cols]
    regions_df = pd.concat([regions_df, region_df])
    df_dict[i] = df_dict[i].drop(columns=region_cols[1:])
  return [df_dict, regions_df]

### Read in Zillow Home Value Index (ZHVI) data
zhvi_metro = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ZillowHomeValueIndex_byMetro.csv')
zhvi_state = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ZillowHomeValueIndex_byState.csv')
zhvi_state.drop(columns=['StateName'], inplace=True)
zhvi_county = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ZillowHomeValueIndex_byCounty.csv')
zhvi_city = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ZillowHomeValueIndex_byCity.csv')
zhvi_zip = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ZillowHomeValueIndex_byZIPCode.csv')
zhvi_neighborhood = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ZillowHomeValueIndex_byNeighborhood.csv')
zhvi = {
    "zhvi_metro": zhvi_metro,
    "zhvi_state": zhvi_state,
    "zhvi_county": zhvi_county,
    "zhvi_city": zhvi_city,
    "zhvi_zip": zhvi_zip,
    "zhvi_neighborhood": zhvi_neighborhood
}

zhvi_metro.columns[5:]

zhvi_processed_all = preprocess_raw(zhvi, [5, 4, 9, 8, 9, 9])
zhvi_processed = zhvi_processed_all[0]
zhi_regions = zhvi_processed_all[1]
zhvi_processed['zhvi_state']

zhi_regions

# Drop unneeded columns from ZHVI regions table
zhi_regions.drop(columns=['State', 'StateCodeFIPS', 'MunicipalCodeFIPS'], inplace = True)

### Read in Zillow Observed Rent Index (ZORI)
zori_metro = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ZillowObservedRentIndex_byMetro.csv')
zori_county = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ZillowObservedRentIndex_byCounty.csv')
zori_city = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ZillowObservedRentIndex_byCity.csv')
zori_zip = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ZillowObservedRentIndex_byZIPCode.csv')
zori = {
    "zori_metro": zori_metro,
    "zori_county": zori_county,
    "zori_city": zori_city,
    "zori_zip": zori_zip
}

zori['zori_metro']

zori_processed_all = preprocess_raw(zori, [5, 9, 8, 9])
zori_processed = zori_processed_all[0]
zori_regions = zori_processed_all[1]

zori_processed['zori_metro']

zori_regions

zori_regions.drop(columns=['State','StateCodeFIPS', 'MunicipalCodeFIPS'], inplace = True)

zori_regions

### Read in Zillow Home Value Forecast (ZHVF). This doesn't need to be preprocessed because the dates are in a fine format, [end of month, end of quarter, +1 year]
zhvf_metro = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ZillowHomeValueForecast_byMetro.csv')
zhvf_metro_regions = zhvf_metro[zhvf_metro.columns[:5]]
zhvf_metro = zhvf_metro.drop(columns=zhvf_metro_regions.columns[1:])
zhvf_metro.columns = ['RegionID', 'BaseDate', 'Month', 'Quarter', 'Year']
zhvf_zip = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ZillowHomeValueForecast_byZIPCode.csv')
zhvf_zip_regions = zhvf_zip[zhvf_zip.columns[:9]]
zhvf_zip = zhvf_zip.drop(columns=zhvf_zip_regions.columns[1:])
zhvf_zip.columns = ['RegionID', 'BaseDate', 'Month', 'Quarter', 'Year']

zhvf_regions = pd.concat([zhvf_metro_regions, zhvf_zip_regions])
zhvf = {
    "zhvf_metro": zhvf_metro,
    "zhvf_zip": zhvf_zip
}

# @title
zhvf_metro

zhvf_zip

### Read in For Sale listings
forSaleListings = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/ForSaleListings_byMetro.csv')
### Read in Market Heat Index
mhi = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/MarketHeatIndex_byMetro.csv')
### Read in New Construction Sales
new_conSales = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/NewConstructionSales_byMetro.csv')
### Read in Sales
sales = pd.read_csv('/content/drive/MyDrive/Zillow_Housing_Data/Sales_byMetro.csv')

### Process remaining data
forSaleListings_processed_all = preprocess_raw({'metro':forSaleListings}, [5])
forSaleListings_processed = forSaleListings_processed_all[0]
forSaleListings_regions = forSaleListings_processed_all[1]

mhi_processed_all = preprocess_raw({'metro':mhi}, [5])
mhi_processed = mhi_processed_all[0]
mhi_regions = mhi_processed_all[1]

newConSales_processed_all = preprocess_raw({'metro':new_conSales}, [5])
newConSales_processed = newConSales_processed_all[0]
newConSales_regions = newConSales_processed_all[1]

sales_processed_all = preprocess_raw({'metro':sales}, [5])
sales_processed = sales_processed_all[0]
sales_regions = sales_processed_all[1]

### Make final regions table
regions = pd.concat([zhi_regions, zori_regions, zhvf_regions, forSaleListings_regions, mhi_regions, newConSales_regions, sales_regions])
regions = regions.drop_duplicates(subset=['RegionID'])

np.unique(regions.RegionType)

regions.to_csv('/content/drive/MyDrive/Zillow_Housing_Data/Regions_cleaned.csv')

### Write data
regions.to_csv('/content/drive/MyDrive/Zillow_Housing_Data/Regions_cleaned.csv')
all_processed = {'zhvi_processed':zhvi_processed, 'zori_processed':zori_processed, 'zhvf':zhvf, 'forSaleListings_processed':forSaleListings_processed, 'mhi_processed':mhi_processed, 'newConSales_processed':newConSales_processed, 'sales_processed':sales_processed}
for d in all_processed:
  curr = all_processed[d]
  for k in curr:
    f = '/content/drive/MyDrive/Zillow_Housing_Data/{}_by-{}_cleaned.csv'.format(d,k)
    print(f)
    curr[k].to_csv(f)

"""#### Entity Resolution
We can safely assume that the Region IDs used for each locality level are consistent between datasets, as all of these datasets were obtained from the same source.
"""